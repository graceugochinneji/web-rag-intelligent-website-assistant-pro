{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM9fi6l4VDKcb0AZStHBSre",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graceugochinneji/web-rag-intelligent-website-assistant-pro/blob/master/Webscraping_%2B_RAG_AI_Assistant_for_Education.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Papxk1CF901o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYy10uIN9gZu"
      },
      "outputs": [],
      "source": [
        "# Install required libraries:\n",
        "# - langchain-community: community tools/integrations for LangChain\n",
        "# - langchain-text-splitters: split docs into chunks for embeddings/RAG\n",
        "# - chromadb: vector database for storing/retrieving embeddings\n",
        "# - sentence-transformers: pretrained models for text embeddings\n",
        "# - beautifulsoup4: parse/extract text from HTML/XML\n",
        "!pip -q install -U langchain-community langchain-text-splitters chromadb sentence-transformers beautifulsoup4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SS_3t0gf9ztJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crawl multiple pages on educosys.com, extract readable text, and keep only useful pages.\n",
        "\n",
        "from langchain_community.document_loaders import RecursiveUrlLoader\n",
        "from bs4 import BeautifulSoup\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "# Seed URL to start crawling and how deep to follow links on the same site.\n",
        "START_URL = \"https://www.educosys.com\"\n",
        "MAX_DEPTH = 2  # Increase to 3–4 to cover more pages (more time/requests).\n",
        "\n",
        "def bs4_extractor(html: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert raw HTML into clean, human-readable text.\n",
        "    - Parses HTML with BeautifulSoup.\n",
        "    - Removes non-content elements (scripts, styles, headers/footers/nav).\n",
        "    - Joins text with newlines and collapses blank lines/whitespace.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    for t in soup([\"script\", \"style\", \"noscript\", \"header\", \"footer\", \"nav\"]):\n",
        "        t.decompose()\n",
        "    txt = soup.get_text(separator=\"\\n\")\n",
        "    return \"\\n\".join(ln.strip() for ln in txt.splitlines() if ln.strip())\n",
        "\n",
        "# RecursiveUrlLoader:\n",
        "# - Starts at START_URL and follows internal links up to MAX_DEPTH.\n",
        "# - Uses our bs4_extractor to clean each page to plain text.\n",
        "# - Stays on the same domain (prevent_outside=True).\n",
        "# - use_async=False is important in notebooks to avoid asyncio event-loop errors.\n",
        "# - Excludes obvious non-HTML/binary paths (images, zip, etc.) by simple substring checks.\n",
        "loader = RecursiveUrlLoader(\n",
        "    url=START_URL,\n",
        "    max_depth=MAX_DEPTH,\n",
        "    extractor=bs4_extractor,\n",
        "    prevent_outside=True,     # don't leave educosys.com\n",
        "    use_async=False,          # safer in Colab/Jupyter (no asyncio.run inside a running loop)\n",
        "    timeout=30,               # per-request timeout (seconds)\n",
        "    check_response_status=True,\n",
        "    exclude_dirs=[\"/logout\", \".pdf\", \".zip\", \".jpg\", \".jpeg\", \".png\", \".svg\", \".gif\", \".ico\"],\n",
        ")\n",
        "\n",
        "# Fetch all raw pages found by the crawler (as LangChain Document objects).\n",
        "docs_raw = loader.load()\n",
        "\n",
        "# Post-filter the documents:\n",
        "# - Keep only URLs that start with the target domain.\n",
        "# - Drop binaries by extension (belt-and-suspenders).\n",
        "# - Require a minimum amount of text (e.g., > 30 words) to skip tiny/empty pages.\n",
        "docs = [\n",
        "    d for d in docs_raw\n",
        "    if d.metadata.get(\"source\", \"\").startswith(\"https://www.educosys.com\")\n",
        "    and not re.search(r\"\\.(pdf|zip|jpg|jpeg|png|svg|gif|ico)$\", d.metadata.get(\"source\", \"\"), re.I)\n",
        "    and len(d.page_content.split()) > 30\n",
        "]\n",
        "\n",
        "print(f\"Kept {len(docs)} pages\")\n"
      ],
      "metadata": {
        "id": "yXABXyXTUb3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter  # splits long docs into manageable, overlapping chunks (good for RAG)\n",
        "\n",
        "# Create a splitter that prefers to cut on paragraphs/lines/words before falling back to characters.\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1200,    # max characters per chunk (≈ 300 tokens; tune to your model/context window)\n",
        "    chunk_overlap=150,  # characters of overlap between consecutive chunks to preserve context continuity\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # split priority: paragraph → line → word → character\n",
        ")\n",
        "\n",
        "# Split a list of LangChain Documents into chunked Documents with updated metadata\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "print(f\"{len(chunks)} chunks\")  # show how many chunks were produced\n"
      ],
      "metadata": {
        "id": "SqH8jWzxAlWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chunks[0])\n",
        "print(chunks[1])\n",
        "print(chunks[2])"
      ],
      "metadata": {
        "id": "fsIk2CwsAlpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{len(chunks)} chunks\")"
      ],
      "metadata": {
        "id": "cX60u6JHArM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U langchain-community langchain-text-splitters langchain-chroma sentence-transformers chromadb beautifulsoup4\n"
      ],
      "metadata": {
        "id": "nNw86adXArd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings    # local (free) sentence-transformer embeddings\n",
        "from langchain_community.vectorstores import Chroma                 # vector DB for storing/retrieving embeddings\n",
        "# NOTE: In newer LangChain, prefer: `from langchain_chroma import Chroma`\n",
        "\n",
        "# Instantiate an embeddings model.\n",
        "# - all-MiniLM-L6-v2: lightweight, fast, good general-purpose semantic search.\n",
        "# - model_kwargs selects GPU in Colab Pro (set to \"cpu\" if no GPU enabled).\n",
        "emb = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={\"device\": \"cuda\"}  # change to \"cpu\" if you didn't enable a GPU runtime\n",
        ")\n",
        "\n",
        "# Build (or append to) a Chroma collection from your chunked documents.\n",
        "# - collection_name: logical name of your dataset inside Chroma.\n",
        "# - persist_directory: on-disk folder; allows reusing the index across sessions.\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunks,                # list[Document] produced by your splitter\n",
        "    embedding=emb,                   # embedding function defined above\n",
        "    collection_name=\"educosys\",      # name of your collection\n",
        "    persist_directory=\"chroma_db\"    # path to store the index files\n",
        ")\n",
        "\n",
        "# Create a retriever interface for RAG.\n",
        "# - k=5: return top-5 most similar chunks per query (tune based on quality/latency).\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n"
      ],
      "metadata": {
        "id": "6lZMhJN3Arrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Works only inside Google Colab: provides no-key access to several LLMs.\n",
        "from google.colab import ai\n",
        "\n",
        "def rag_answer(question, k=5, max_chars=1200):\n",
        "    \"\"\"\n",
        "    Retrieve top-k relevant chunks from your Chroma index and ask Colab's LLM\n",
        "    to answer *using only that context*.\n",
        "\n",
        "    Args:\n",
        "        question (str): user question (e.g., \"How many LLM courses are there?\")\n",
        "        k (int): number of chunks to include in the prompt (defaults to 5)\n",
        "        max_chars (int): max characters per chunk to keep prompt compact\n",
        "    Returns:\n",
        "        reply (str): model's answer text\n",
        "        docs  (List[Document]): retrieved documents (for debugging/attribution)\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve similar chunks from your vector store (top-k by similarity).\n",
        "    docs = retriever.get_relevant_documents(question)\n",
        "\n",
        "    # Edge case: nothing retrieved (empty index or no match).\n",
        "    if not docs:\n",
        "        return \"I couldn't find relevant context in the index.\", []\n",
        "\n",
        "    # Build a compact, traceable context block limited to k docs and max_chars each.\n",
        "    context = \"\\n\\n\".join(\n",
        "        f\"[{i+1}] {d.metadata.get('source','')}\\n{d.page_content[:max_chars]}\"\n",
        "        for i, d in enumerate(docs[:k])  # ← respect k\n",
        "    )\n",
        "\n",
        "    # Constrain the model to ground its answer in the provided context only.\n",
        "    prompt = (\n",
        "        \"You are a helpful assistant. Answer using ONLY the context.\\n\\n\"\n",
        "        f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "    # Generate an answer via Colab AI (no API key needed on Pro/Pro+ plans).\n",
        "    reply = ai.generate_text(prompt)\n",
        "\n",
        "    return reply, docs\n",
        "\n",
        "# Example call (note: 'course' is spelled correctly)\n",
        "answer, sources = rag_answer(\"How many LLM courses are there?\")\n",
        "print(answer)\n",
        "\n",
        "print(\"\\nSources:\")\n",
        "for s in sources:\n",
        "    print(\"-\", s.metadata.get(\"source\",\"\"))"
      ],
      "metadata": {
        "id": "2EdjiGkEAxrD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}