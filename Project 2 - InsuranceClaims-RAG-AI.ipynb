{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graceugochinneji/web-rag-intelligent-website-assistant-pro/blob/master/Project%202%20-%20InsuranceClaims-RAG-AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPibjVwfcFk7"
      },
      "outputs": [],
      "source": [
        "# Install required libraries for document processing, embeddings, and vector storage\n",
        "# Core LangChain community integrations\n",
        "# For splitting documents into smaller text chunks\n",
        "# LangChain wrapper for Chroma vector database\n",
        "# Pre-trained models for creating embeddings\n",
        "# Pre-trained models for creating embeddings\n",
        "# Chroma vector database for storing/retrieving embeddings\n",
        "# PDF parsing and text extraction\n",
        "# Extract text from Microsoft Word (.docx) files\n",
        "# Extract text/content from PowerPoint (.pptx) files\n",
        "# Parse and extract text from HTML/XML documents\n",
        "\n",
        "!pip -q install -U \\ langchain-community \\ langchain-text-splitters \\ langchain-chroma \\ sentence-transformers \\ chromadb \\ pypdf \\ docx2txt \\ python-pptx \\ beautifulsoup4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eca7SGe7esE9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive into the Colab environment\n",
        "# This will prompt you to authorize access the first time you run it.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ðŸ‘‡ Define the path to your dataset directory in Google Drive\n",
        "# Change this only if your dataset is saved in a different folder.\n",
        "DATA_DIR = \"/content/drive/MyDrive/ml_dataset\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive into the Colab environment.\n",
        "# This will create a link for you to authorize access the first time.\n",
        "# After mounting, your Drive files will be accessible under '/content/drive'.\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "q91hoHS756O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRvEwajefBup"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from typing import List\n",
        "import re\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
        "from pptx import Presentation  # python-pptx\n",
        "\n",
        "# ----------------------------\n",
        "# Load PDF documents\n",
        "# ----------------------------\n",
        "def load_pdf(path: Path) -> List[Document]:\n",
        "    # PyPDFLoader extracts one Document per page\n",
        "    loader = PyPDFLoader(str(path))\n",
        "    docs = loader.load()\n",
        "    for d in docs:\n",
        "        # Add useful metadata to each page\n",
        "        d.metadata.update({\n",
        "            \"source\": str(path),       # full file path\n",
        "            \"filename\": path.name,     # just the file name\n",
        "            \"ext\": path.suffix.lower(),# file extension\n",
        "            \"week\": path.parent.name   # parent folder (e.g., \"Week 1\")\n",
        "        })\n",
        "    return docs\n",
        "\n",
        "# ----------------------------\n",
        "# Load DOCX documents\n",
        "# ----------------------------\n",
        "def load_docx(path: Path) -> List[Document]:\n",
        "    # Docx2txtLoader extracts all text as a single Document\n",
        "    loader = Docx2txtLoader(str(path))\n",
        "    docs = loader.load()\n",
        "    for d in docs:\n",
        "        d.metadata.update({\n",
        "            \"source\": str(path),\n",
        "            \"filename\": path.name,\n",
        "            \"ext\": path.suffix.lower(),\n",
        "            \"week\": path.parent.name\n",
        "        })\n",
        "    return docs\n",
        "\n",
        "# ----------------------------\n",
        "# Load PPTX documents\n",
        "# ----------------------------\n",
        "def load_pptx(path: Path) -> List[Document]:\n",
        "    prs = Presentation(str(path))  # open PowerPoint\n",
        "    docs = []\n",
        "    for i, slide in enumerate(prs.slides, start=1):\n",
        "        chunks = []\n",
        "        # Collect all text from shapes on each slide\n",
        "        for shape in slide.shapes:\n",
        "            if hasattr(shape, \"has_text_frame\") and shape.has_text_frame:\n",
        "                txt = \"\\n\".join(\n",
        "                    p.text for p in shape.text_frame.paragraphs if p.text\n",
        "                )\n",
        "                if txt.strip():\n",
        "                    chunks.append(txt.strip())\n",
        "        # Join the slideâ€™s text into one chunk\n",
        "        slide_text = \"\\n\".join(chunks).strip()\n",
        "        if slide_text:\n",
        "            docs.append(\n",
        "                Document(\n",
        "                    page_content=slide_text,\n",
        "                    metadata={\n",
        "                        \"source\": str(path),\n",
        "                        \"filename\": path.name,\n",
        "                        \"ext\": path.suffix.lower(),\n",
        "                        \"slide\": i,             # slide number\n",
        "                        \"week\": path.parent.name\n",
        "                    }\n",
        "                )\n",
        "            )\n",
        "    return docs\n",
        "\n",
        "# ----------------------------\n",
        "# Load all documents from a directory\n",
        "# ----------------------------\n",
        "def load_all(base_dir: str) -> List[Document]:\n",
        "    base = Path(base_dir)\n",
        "    all_docs: List[Document] = []\n",
        "    for p in base.rglob(\"*\"):\n",
        "        if not p.is_file():\n",
        "            continue\n",
        "        ext = p.suffix.lower()\n",
        "        try:\n",
        "            if ext == \".pdf\":\n",
        "                all_docs += load_pdf(p)\n",
        "            elif ext == \".docx\":\n",
        "                all_docs += load_docx(p)\n",
        "            elif ext == \".pptx\":\n",
        "                all_docs += load_pptx(p)\n",
        "            # Other file types are ignored\n",
        "        except Exception as e:\n",
        "            print(f\"[skip] {p.name}: {e}\")\n",
        "    return all_docs\n",
        "\n",
        "# ----------------------------\n",
        "# Run the loader\n",
        "# ----------------------------\n",
        "docs = load_all(DATA_DIR)\n",
        "\n",
        "# Optional: filter out empty or very short pages (<10 words)\n",
        "docs = [d for d in docs if len(d.page_content.split()) > 10]\n",
        "\n",
        "print(f\"Loaded {len(docs)} Documents\")\n",
        "\n",
        "# Quick peek at first 5 docs (metadata + preview text)\n",
        "for d in docs[:5]:\n",
        "    print(d.metadata, \"â†’\", d.page_content[:120].replace(\"\\n\",\" \"), \"â€¦\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Z8vSWUVfD2a"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# ----------------------------------------\n",
        "# STEP 1: Split documents into smaller chunks\n",
        "# ----------------------------------------\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,          # Maximum number of characters per chunk\n",
        "    chunk_overlap=150,        # Overlap between chunks (helps retain context continuity)\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Priority order for splitting text\n",
        ")\n",
        "chunks = splitter.split_documents(docs)\n",
        "print(f\"Chunks: {len(chunks)}\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# STEP 2: Load Embedding Model\n",
        "# ----------------------------------------\n",
        "import torch\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# Use GPU if available, otherwise fallback to CPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# HuggingFace sentence-transformer for embedding text into vectors\n",
        "emb = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",  # Lightweight, fast, good quality\n",
        "    model_kwargs={\"device\": device}                      # Run on GPU/CPU as detected\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# STEP 3: Create Vector Database with Chroma\n",
        "# ----------------------------------------\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunks,           # Chunked documents to embed & index\n",
        "    embedding=emb,              # Embedding model\n",
        "    collection_name=\"project_two\",   # Name of your vector collection\n",
        "    persist_directory=\"chroma_db\"    # Local directory for saving index\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# STEP 4: Setup Retriever for Search\n",
        "# ----------------------------------------\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "# \"k=5\" â†’ return top 5 most relevant chunks for each query\n",
        "\n",
        "print(\"Chroma index ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import ai\n",
        "\n",
        "# ----------------------------------------\n",
        "# Utility function to wrap long text\n",
        "# ----------------------------------------\n",
        "def wrap_text(text, words_per_line=15):\n",
        "    \"\"\"\n",
        "    Breaks text into lines of a fixed number of words\n",
        "    for easier readability in Colab outputs.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to wrap.\n",
        "        words_per_line (int): Number of words before inserting a line break.\n",
        "\n",
        "    Returns:\n",
        "        str: Wrapped text with line breaks.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    lines = []\n",
        "    # Create chunks of 'words_per_line'\n",
        "    for i in range(0, len(words), words_per_line):\n",
        "        lines.append(\" \".join(words[i:i+words_per_line]))\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# Main Q&A function\n",
        "# ----------------------------------------\n",
        "def ask(question, k=5, max_chars=1200):\n",
        "    \"\"\"\n",
        "    Retrieves context documents, builds a prompt,\n",
        "    queries the AI model, and wraps the answer.\n",
        "\n",
        "    Args:\n",
        "        question (str): The user question.\n",
        "        k (int): Number of top documents to retrieve.\n",
        "        max_chars (int): Maximum characters of each document to include in context.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (wrapped answer string, list of context documents)\n",
        "    \"\"\"\n",
        "    # Retrieve top-k relevant documents\n",
        "    ctx_docs = retriever.get_relevant_documents(question)\n",
        "\n",
        "    # Build the context string (include file + week metadata)\n",
        "    context = \"\\n\\n\".join(\n",
        "        f\"[{i+1}] {d.metadata.get('filename')} ({d.metadata.get('week')})\\n{d.page_content[:max_chars]}\"\n",
        "        for i, d in enumerate(ctx_docs)\n",
        "    )\n",
        "\n",
        "    # Construct the AI prompt\n",
        "    prompt = (\n",
        "        \"You are a helpful assistant. Answer ONLY from the context.\\n\\n\"\n",
        "        f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "    # Generate AI response\n",
        "    result = ai.generate_text(prompt)\n",
        "\n",
        "    # Wrap the response text for readability\n",
        "    return wrap_text(result), ctx_docs\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# Example usage\n",
        "# ----------------------------------------\n",
        "answer, sources = ask(\"What is the differences between week 1 and week 2 content?\")\n",
        "\n",
        "# Print formatted answer\n",
        "print(answer)\n",
        "\n",
        "# Print document sources used for the answer\n",
        "print(\"\\nSources:\")\n",
        "for s in sources:\n",
        "    print(\"-\", s.metadata.get(\"filename\"), \"|\", s.metadata.get(\"week\"))\n"
      ],
      "metadata": {
        "id": "z0HTVKP088at"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}